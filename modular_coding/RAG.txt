RAG -
* Most useful for...
=> private data
=> Advanced topics the LLM might not have seen during training.

Components in a RAG that needs to be evaluated...
* Retrieval Component
* Answer generation Component


=> Retrieval Metrics focuses on how well the system retrieves the relevant information from its
knowledge base
1. precision@k
2. Recall@k,F1@k
3. Hit Rate - Proportion of queries for which atleast one relevant document is retrieved with the top few results

---------------------------------------------------------------------------------------------

4.Mean Reciprocal Rank (MRR)- 
How high the relevant document appears in the search results.
(Useful for systems where users prioritize top results)

MRR = (1/Q)*Sum(1/Rank of the first relevant document), where Q is the total number of queries


----------------------------------------------------------------------------------


5. Normalized Discounted Cumulative Gain (NDCG@k)-
- Takes relevance and ranking of retrieved documents into account.
(Useful when order of results matters)

# Example: nDCG Calculation

# Scenario:
# For a query, the system retrieves 5 documents with the following relevance scores (ground truth):
# Retrieved: [3, 2, 3, 0, 1] (relevance scores for documents at positions 1 to 5).
# Ideal Order: [3, 3, 2, 1, 0] (relevance scores in the ideal ranking).

# Step-by-Step Calculation

# DCG Calculation:
# DCG_5 = 3 + (2 / log2(3)) + (3 / log2(4)) + (0 / log2(5)) + (1 / log2(6))
# DCG_5 ≈ 3 + 1.26 + 1.0 + 0 + 0.39 = 5.65

# IDCG Calculation:
# IDCG_5 = 3 + (3 / log2(3)) + (2 / log2(4)) + (1 / log2(5)) + (0 / log2(6))
# IDCG_5 ≈ 3 + 1.89 + 1.0 + 0.43 + 0 = 6.32

# nDCG Calculation:
# nDCG_5 = DCG_5 / IDCG_5 = 5.65 / 6.32 ≈ 0.894

# Final Results:
# DCG (Retrieved): 5.65
# IDCG (Ideal): 6.32
# nDCG: 0.894



-------------------------------------------------------------------------------------------------

6. Mean Average Precision (MAP@k)

MAP ensures the model prioritizes relevant items over irrelevant ones.
MAP provides an average precision over multiple queries or search tasks, 
reflecting the system's overall ranking capability.

Example calculation...

Scenario:
You have 2 queries, each with a ranked list of documents. Relevance is binary (1 for relevant, 0 for irrelevant):

- Query 1:  
  Ranked List: [1, 0, 1, 1]  ; 0 => Relevant , 1 => Irrelevant
  Total Relevant Items: 3  

- Query 2:  
  Ranked List: [0, 1, 0, 1]  
  Total Relevant Items: 2  

---

Step 1: Calculate Average Precision (AP) for Each Query

Query 1:
- Precision@1 = 1/1 = 1.0 (Relevant)  
- Precision@2 = 1/2 = 0.5 (Irrelevant)  
- Precision@3 = 2/3 ≈ 0.667 (Relevant)  
- Precision@4 = 3/4 = 0.75 (Relevant)  

AP(Query 1) = (1.0 + 0.667 + 0.75) / 3 ≈ 0.806  

Query 2:
- Precision@1 = 0/1 = 0.0 (Irrelevant)  
- Precision@2 = 1/2 = 0.5 (Relevant)  
- Precision@3 = 1/3 ≈ 0.333 (Irrelevant)  
- Precision@4 = 2/4 = 0.5 (Relevant)  

AP(Query 2) = (0.5 + 0.5) / 2 = 0.5  

---

Step 2: Calculate Mean Average Precision (MAP)

MAP = (AP(Query 1) + AP(Query 2)) / 2  
MAP = (0.806 + 0.5) / 2  
MAP = 0.653  

---

Final Result:
MAP = 0.653



