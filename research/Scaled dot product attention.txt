If we don't scale the attention scores(Q @ K), then it can result in peaky SoftMax due to the nature of dot product (for high dimensional vectors)! 
Which means...there is higher variance of similarity scores(some are very high , some are very low).This can create peaky SoftMax.

But, How exactly does SoftMax being â€œpeakyâ€ make gradients flow to only some values?

output = âˆ‘ Î±áµ¢ váµ¢
L = loss(output)

Then by chain rule:
âˆ‚L/âˆ‚váµ¢ = âˆ‚L/âˆ‚output Ã— âˆ‚output/âˆ‚váµ¢
        = âˆ‚L/âˆ‚output Ã— Î±áµ¢  [Note: âˆ‚output/âˆ‚váµ¢ is just Î±áµ¢ Ã— Iâ‚™ (identity matrix)]
        = Î±áµ¢ Ã— âˆ‚L/âˆ‚output


So if Î±áµ¢ â‰ˆ 0, that value vector váµ¢ receives no gradient, i.e., no learning.
Thatâ€™s why other váµ¢ donâ€™t learn â€” theyâ€™re not contributing to the output, and theyâ€™re not receiving signal to change.
Thatâ€™s why softmax peaking = bad gradient flow ( localized gradient starvation or sparse gradient flow due to peaky attention)

â€œIn peaky attention, a few value vectors (typically one) receive most of the gradient signal, while others get almost nothing â€” leading to limited or biased learning.â€


ğŸ’¡ Why does this matter?

If your attention always peaks hard, your model:

* Learns to overfit to some tokens

* Doesnâ€™t explore contextual diversity

* Misses richer interactions between tokens


âœ… Scaling by âˆšd helps prevent this:

softmax(q Â· káµ€ / âˆšd)

* The values stay in a moderate range

* Softmax doesnâ€™t saturate

* Attention is spread across more tokens

* Gradients flow more evenly

